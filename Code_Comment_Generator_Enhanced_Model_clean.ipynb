{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4q46FUbJi8I"
   },
   "source": [
    "## This notebook aims to fine-tune CodeT5 for generating descriptive comments for code snippets, aiding developers in maintaining readable codebases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVEq6RwwEdgv"
   },
   "source": [
    "### PHASE 1: Setup and Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXt1QR2QzFmD",
    "outputId": "392e4423-6c7c-4ece-c9ff-6b318363cffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCcceGcQDwOv",
    "outputId": "48065229-863c-46c5-8dd2-9d239765c58c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0f08322d3a4b3c1dd0cd9ea9cfd2a37d0c99f401bdd1314a9d16a2a7e9add44b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score, evaluate\n",
      "Successfully installed evaluate-0.4.5 rouge_score-0.1.2\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading transformers-4.54.0-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.4\n",
      "    Uninstalling huggingface-hub-0.33.4:\n",
      "      Successfully uninstalled huggingface-hub-0.33.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.3\n",
      "    Uninstalling transformers-4.53.3:\n",
      "      Successfully uninstalled transformers-4.53.3\n",
      "Successfully installed huggingface-hub-0.34.1 transformers-4.54.0\n",
      "Collecting datasets==3.0.0\n",
      "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (0.70.16)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.0)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (3.12.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (0.34.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.22.0->datasets==3.0.0) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.22.0->datasets==3.0.0) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.0.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.0.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.0.0) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.0) (1.17.0)\n",
      "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.0.0 fsspec-2024.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate sentencepiece rouge_score\n",
    "!pip install --upgrade transformers\n",
    "!pip install datasets==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FtyBIhJKIJ8b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46zgEdeYJevy"
   },
   "source": [
    "### PHASE 2: Dataset Acquisition & Preprocessing\n",
    "Objective: Use the CodeSearchNet Python subset and format it for CodeT5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "44202444368f4da090dc4ad2a59d03d4",
      "e71fa86e62a14ebe8c6d7c6ae473ec83",
      "a48b37a292b8480f93c558d64ce5a5ef",
      "1dd5e8d9d47b4a648e591ad8e9dd1995",
      "8c5b05046c3d4be0ab78d28ee3a2d6df",
      "49b948d3026e49d5bc273d702c418c91",
      "aa7aceddec2d49eb8e43a294622d719e",
      "65342ddbb9c6453ba15a4debb4722400",
      "947870b1b8b04e9ea8b6348d97097dfe",
      "343392c5d6564780a53ffa9e84987da3",
      "ceff475345e846bf8be87239ebb36c50",
      "4c3346363e794c2e90a80033fafce51c",
      "f9cff72caa874dedb715fc0042524103",
      "ad5035fee5074b058eaef39e36c421d9",
      "d8d7083f90a54122bd2b39549c92b43e",
      "bdaa8185334b4f5ebb9da71e5f6e3605",
      "eccad0bc4e4446828e7bc2287e251023",
      "cc4cdc88044741678747498beed0187b",
      "7978b8de474d497eb5fd652fd59424bc",
      "1a18ab62af194101917f02f890e84f8c",
      "51a15d58a06540fbb5cff1f0206c914e",
      "3ad6c54b671a4f689a3883fd83ac1fd4",
      "51554bf78c714f53b508e21970341327",
      "8308cecd7a594f01bbcef60721509174",
      "0b090baba11e486da583e404cdbace13",
      "ba7a7a6c897044f38cc60316fa9726c2",
      "8a0c035eea1f4b3aaaf896218a8dd22f",
      "fa4f6020fbb74d919eec97506dfee5e4",
      "5b4324aac32446279bf5065bdaba3554",
      "6b018c5a956648e4ae775c0b0b18c530",
      "ba28fdf605dd43c8bf3c61be2b3e9393",
      "cfb5720ba7574f1d9ba67c2049bbe1ac",
      "62d5d235a8c446008e4f7414165a0136",
      "0745f3fcfb6b4371a3acca08984052a9",
      "a7ebdc7d2a354e49bf019fd13461a289",
      "e8cf8168fbf547c2bbffe3f2ba16fc3a",
      "3706afbb389b43228ca97de20710ce4d",
      "8a096ffc214c425e9447c91120177cc0",
      "a60366aef24c4868b1089330ec00159a",
      "d6d6db20510546f984cf4ddc7673200c",
      "f038289e7fad44a49b58f74cec054c5e",
      "08486be9314344e78646a7cb4d7c8887",
      "953a0f059966478590be134c89db7767",
      "0076cd1064734403a1f09f601e1a309a",
      "e14808330b9c4fa886b5eb9f2a231ab9",
      "993d0a84fd2b42f4aaad1b17c9015abe",
      "1fb31c8026a34c4c96a99a21a6e70c3c",
      "c87c00eca60f4eceb452a7f1e7719ca4",
      "7c5bcf1d2b064f65883e217c0738d944",
      "5aea8a6dcf51468781e94f88ee995655",
      "9c7e77cc6e66447895aeb8765cfa70d9",
      "0a58990cba82464abb54b0501f0ea3b4",
      "a0c06229a10d4e24820b23cc68272037",
      "0bb927fde1f84015a02d2d72c1f33912",
      "a1329459c9df4a6999eb623ec4347d87",
      "d2d3ba6476304df8b82bd1151112d493",
      "9a4aaaac8c4c4e1f8917e03f4a952f58",
      "ceb938649c474541a4e18b465d47bd6d",
      "09fad370bd19468b911e24c0ef778602",
      "65ad88c60c7343c69006bc2ba33dfbcc",
      "56920304117243b1ab29e83bbfc114d9",
      "b03d90fbf6b6442eaf7e72bef3e70129",
      "b8639cd94ff84e3a8fcad0c918078eec",
      "0da17b813eb349f9a4d5b4c09cdb98d0",
      "111ce5903f7143f58e7393da98719d84",
      "c68b95f9f15043a2bdb6793851be6215"
     ]
    },
    "id": "ZMfbPIRRKjdo",
    "outputId": "30f8365b-0db5-4df3-f636-f9d2f06db801"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44202444368f4da090dc4ad2a59d03d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_search_net.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3346363e794c2e90a80033fafce51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51554bf78c714f53b508e21970341327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "python.zip:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0745f3fcfb6b4371a3acca08984052a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14808330b9c4fa886b5eb9f2a231ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d3ba6476304df8b82bd1151112d493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the python subset of CodeSeachNet\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmCH_a72i4sI"
   },
   "source": [
    "#### Filtering Invalid Samples, applying preprocessing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6cQ2_hIrL2-",
    "outputId": "a84fe201-7ce9-4bd9-9571-184e95380fa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 412178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 22176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 23107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVb_Ybgun70e",
    "outputId": "bd62b290-53d9-458d-8470-cc4561a52862"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repository_name',\n",
       " 'func_path_in_repository',\n",
       " 'func_name',\n",
       " 'whole_func_string',\n",
       " 'language',\n",
       " 'func_code_string',\n",
       " 'func_code_tokens',\n",
       " 'func_documentation_string',\n",
       " 'func_documentation_tokens',\n",
       " 'split_name',\n",
       " 'func_code_url']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "wlwNZrQur7cp",
    "outputId": "a9c393da-92f6-4831-e329-9faaab20dc5e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'def ensure_dir(d):\\n    \"\"\"\\n    Check to make sure the supplied directory path does not exist, if so, create it. The\\n    method catches OSError exceptions and returns a descriptive message instead of\\n    re-raising the error.\\n\\n    :type d: str\\n    :param d: It is the full path to a directory.\\n\\n    :return: Does not return anything, but creates a directory path if it doesn\\'t exist\\n             already.\\n    \"\"\"\\n    if not os.path.exists(d):\\n        try:\\n            os.makedirs(d)\\n        except OSError as oe:\\n            # should not happen with os.makedirs\\n            # ENOENT: No such file or directory\\n            if os.errno == errno.ENOENT:\\n                msg = twdd(\"\"\"One or more directories in the path ({}) do not exist. If\\n                           you are specifying a new directory for output, please ensure\\n                           all other directories in the path currently exist.\"\"\")\\n                return msg.format(d)\\n            else:\\n                msg = twdd(\"\"\"An error occurred trying to create the output directory\\n                           ({}) with message: {}\"\"\")\\n                return msg.format(d, oe.strerror)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][3][\"func_code_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "BxGgRD5RsF9j",
    "outputId": "3a4fe337-2298-4752-f2cd-bae8b869ced3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Check to make sure the supplied directory path does not exist, if so, create it. The\\n    method catches OSError exceptions and returns a descriptive message instead of\\n    re-raising the error.\\n\\n    :type d: str\\n    :param d: It is the full path to a directory.\\n\\n    :return: Does not return anything, but creates a directory path if it doesn't exist\\n             already.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][3][\"func_documentation_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e741de960e8948e6b2aa08b39426480d",
      "6cafee1d579f4f97b2a75e2ed5d1f994",
      "8f285479ba37468ebeaf2bf180e916a5",
      "14138ac1a9bc44f0baaec94a1a06c084",
      "436131abd8c340b4accec9b0511d8e4c",
      "89df5cb7c874468283f7db03ee4350e8",
      "491b0aa871cb40e2a94d6fc8c89263c1",
      "6343f1e1bec545e195ccd5930beed2dd",
      "f75564c5b5414a79bff22bc128907b41",
      "ddad38eef6d3434a969015dc5b9723c8",
      "34a79e0cc3cb429292c22e974759acb0"
     ]
    },
    "id": "LNJra7xYmO0Z",
    "outputId": "bcd02342-ff7c-4ca4-cb8e-86b2036bd8dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e741de960e8948e6b2aa08b39426480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filtering Function (has_valid_fields):\n",
    "\n",
    "Ensures code and docstrings are non-empty, sufficiently long (>50 chars for code, >10 for docs), and non-placeholder (no \"TODO\").\n",
    "\n",
    "Excludes overly long docstrings (>5 newlines) to maintain conciseness.\n",
    "\n",
    "Apply Filter:\n",
    "\n",
    "Filters the training split to create a cleaner dataset for fine-tuning, removing low-quality examples.\n",
    "\n",
    "Purpose:\n",
    "Prepares a high-quality dataset for training models (e.g., CodeT5) to generate accurate and relevant code explanations.\n",
    "\"\"\"\n",
    "def has_valid_fields(example):\n",
    "    # More stringent filtering\n",
    "    return (\n",
    "        example[\"func_code_string\"] and\n",
    "        example[\"func_documentation_string\"] and\n",
    "        len(example[\"func_code_string\"].strip()) > 50 and  # Minimum code length\n",
    "        len(example[\"func_documentation_string\"].strip()) > 10 and  # Minimum doc length\n",
    "        not example[\"func_documentation_string\"].startswith(\"TODO\") and\n",
    "        example[\"func_documentation_string\"].count('\\n') < 5  # Avoid overly long docs\n",
    "    )\n",
    "\n",
    "train_dataset = ds[\"train\"].filter(has_valid_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "56161ab52fb74395b36dc76431021c88",
      "cf676633e33e47139573a9dec4e0aa52",
      "1c096a9ec9284880842d9f0282053816",
      "18cf8506f22047208c7d6c5724756828",
      "bbaa51cf957e439097d1efd6475e7393",
      "34847a50132f4a05add292561ad1568f",
      "842496b8e88f4d03b586f870990c39be",
      "a3664f3fa8f44a689ae5a4b05d4c3c4b",
      "67d571ef684a4d0492e09e450bb8dfd7",
      "a6788d20924c4531b7b5cb52a3058fa4",
      "b04733336cde40acb642a6e968dc241f"
     ]
    },
    "id": "4drfAHDOs4GP",
    "outputId": "9c92d226-7784-4688-b8a8-2affd3c35502"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56161ab52fb74395b36dc76431021c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/253254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Docstring Cleaning:\n",
    "\n",
    "Removes whitespace and triple-quotes to ensure clean, consistent docstrings.\n",
    "\n",
    "Structured Input-Target Format:\n",
    "\n",
    "Input: Code prefixed with Generate docstring: (task prompt).\n",
    "\n",
    "Target: Cleaned docstring (expected model output).\n",
    "\n",
    "Purpose: Prepares data for seq2seq training (code → docstring generation).\n",
    "\n",
    "Dataset Processing:\n",
    "\n",
    "Applies process to all examples and removes unused columns to optimize memory and compatibility.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def process(example):\n",
    "    # Clean the docstring\n",
    "    doc = example[\"func_documentation_string\"].strip()\n",
    "    # Remove common prefixes that might confuse the model\n",
    "    doc = doc.replace('\"\"\"', '').replace(\"'''\", '').strip()\n",
    "\n",
    "    return {\n",
    "        \"input_text\": \"Generate docstring: \" + example[\"func_code_string\"],\n",
    "        \"target_text\": doc\n",
    "    }\n",
    "\n",
    "processed_dataset = train_dataset.map(process).remove_columns(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjqfcIhhDHXR",
    "outputId": "1eb5aa96-dbcf-427e-83f6-3c921b9b026a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset...\n",
      "Original dataset size: 253254\n",
      "Augmented dataset size: 1013016\n"
     ]
    }
   ],
   "source": [
    "# Manual augmentation approach\n",
    "\n",
    "\"\"\"\n",
    "- Manual augmentation creates multiple prompt variations for each code snippet (e.g., `\"Explain this function: \"`, `\"Document: \"`).\n",
    "- Goal: Improve model generalization by training it to respond to different phrasings of the same task.\n",
    "\"\"\"\n",
    "def create_augmented_dataset(dataset):\n",
    "    \"\"\"Create an augmented dataset by expanding each example\"\"\"\n",
    "    all_examples = []\n",
    "\n",
    "    for example in dataset:\n",
    "        code = example[\"input_text\"].replace(\"Generate docstring: \", \"\")\n",
    "        target = example[\"target_text\"]\n",
    "\n",
    "        # Create variations\n",
    "        variations = [\n",
    "            {\"input_text\": \"Generate docstring: \" + code, \"target_text\": target},\n",
    "            {\"input_text\": \"Explain this function: \" + code, \"target_text\": target},\n",
    "            {\"input_text\": \"What does this code do: \" + code, \"target_text\": target},\n",
    "            {\"input_text\": \"Document: \" + code, \"target_text\": target},\n",
    "        ]\n",
    "\n",
    "        all_examples.extend(variations)\n",
    "\n",
    "    # Convert back to dataset\n",
    "    from datasets import Dataset\n",
    "    return Dataset.from_list(all_examples)\n",
    "\n",
    "# Apply augmentation\n",
    "print(\"Creating augmented dataset...\")\n",
    "augmented_dataset = create_augmented_dataset(processed_dataset)\n",
    "print(f\"Original dataset size: {len(processed_dataset)}\")\n",
    "print(f\"Augmented dataset size: {len(augmented_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcF62BzDwNRP",
    "outputId": "2cb325ce-6e20-4dc1-cabf-fc25085cf40c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Input:\n",
      " Generate docstring: def _compute_weight_std(self, C, mag):\r\n",
      "        \"\"\"\r\n",
      "        Common part of equations 8 and 9, page 971.\r\n",
      "        \"\"\"\r\n",
      "        if mag < 6.0:\r\n",
      "            return C['a1']\r\n",
      "        elif mag >= 6.0 and mag < 6.5:\r\n",
      "            return C['a1'] + (C['a2'] - C['a1']) * ((mag - 6.0) / 0.5)\r\n",
      "        else:\r\n",
      "            return C['a2']\n",
      "\n",
      "Target:\n",
      " Common part of equations 8 and 9, page 971.\n",
      "\n",
      "--- Example 2 ---\n",
      "Input:\n",
      " Generate docstring: def copy(self, *, shallow=False):\n",
      "        \"\"\"Return a copy of a table.\"\"\"\n",
      "        table = type(self)()\n",
      "        for label in self.labels:\n",
      "            if shallow:\n",
      "                column = self[label]\n",
      "            else:\n",
      "                column = np.copy(self[label])\n",
      "            self._add_column_and_format(table, label, column)\n",
      "        return table\n",
      "\n",
      "Target:\n",
      " Return a copy of a table.\n",
      "\n",
      "--- Example 3 ---\n",
      "Input:\n",
      " Generate docstring: def run_display_description(self):\n",
      "        \"\"\"Print profile name with programMain.\"\"\"\n",
      "        # display description if available\n",
      "        if self.profile.get('description'):\n",
      "            print(\n",
      "                'Description: {}{}{}'.format(\n",
      "                    c.Style.BRIGHT, c.Fore.MAGENTA, self.profile.get('description')\n",
      "                )\n",
      "            )\n",
      "\n",
      "Target:\n",
      " Print profile name with programMain.\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(processed_dataset.shuffle(seed=42).select(range(3))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(\"Input:\\n\", example[\"input_text\"][:500])\n",
    "    print(\"\\nTarget:\\n\", example[\"target_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264,
     "referenced_widgets": [
      "d0534fcbdca847be89ac18fbf101ac98",
      "175626e4e6f241378ca01b3d4ea0f3e9",
      "a04b6bdc3d4442d594a466be1a3b4b5b",
      "3b9f8d16a1fe427188ed5bd07d6316d6",
      "6a5d7f9f1f244e7fbeac3389ac562eb9",
      "71bc8a0b39bf4be19b0a79e0b0947ef8",
      "e1aa056fe0e54d5184b7f5df1828a86a",
      "f638466920044952a7147b05de25d022",
      "3ca146cfa0774d42acc947f6995704fb",
      "08d5be2f9c6040e88ddfee8930c64076",
      "a1e118317feb492f89bcb9cc2154b098",
      "b29a3b77bda84ae78173448d4a23bcf5",
      "cb89acbb40874790916ce1314a2df076",
      "966afe8e0ef240ed8ccfd513ef0996e8",
      "a9052608e86042da85eb3d30d3602396",
      "96144d9d13da41a69fac3b82364c22aa",
      "caeb402009524b72b61927fd93c5224f",
      "f0f2ee07aed94b17adda92ea635bd5db",
      "1ebeb450bc614a44871571c96dc62f87",
      "c394ace6726d4293a954dad513934db5",
      "6a814811a8794c9eaeac9c47114811fd",
      "9a59bc404bd24bfc81296250387fc5cf",
      "e67dbc6157464b05993e2ff8e581c2f7",
      "5df452cd50c34a569835fd8870600422",
      "c643ba2e23d3410c8c4ee554fb307678",
      "a8a1206849cb436d83e598f930c894dd",
      "09243edf02584f1796d934dad9ebc89a",
      "e838d9f042014579a12e5e763f466d2c",
      "e37bc4f963064a6997a94dd2c924ee11",
      "8cf755e73b384e7cbaee1b42a3251cec",
      "b59e0ef90afb42fe8b459bca08de1949",
      "4507a6d03e0c49e5ba816e3176a71baa",
      "3cca547cafc346538dcbb82362317b5d",
      "b2c8f37345ea4b6292f8f0c8a9f7b97e",
      "e92b84fb68e94758be4f72151cd82f86",
      "b9c81110538a4a63a8b2140f91ae9aa7",
      "63a149f6651a4c69bf6969a7a6ad0cc5",
      "e412147da4004afb9df3dfff13083c99",
      "6bbbdc7592fb4300b88df7214c8115a7",
      "cb7a75dc68b443449fa69673e43d80a3",
      "f8d8ecb7782844b7b7049b4d09adb633",
      "f1bebc94e3c6412d9e1e2cb9ea728ce8",
      "3347907841be445eb406174b5d99b494",
      "fb67c6b74a374cfda6ed336b006837ac",
      "34a4e4cf1a4147c98540c251ce34efc4",
      "1f36388493e04aedbb858a239b423b7c",
      "e6bc3d65d95e444d817df4e079abed94",
      "27943b024042486ab5bbb139a755f457",
      "0895b85df03148359cb1521e880a366b",
      "e29d84b67109417e8104d88bfebaeaa1",
      "a96f6bfc8fa24f74bcf545248cb2a722",
      "13f73a9448954d62b7c32a460bd4ba3f",
      "4a3e72d077a94c9d9db4da0d2bcb20d1",
      "b10dd7ae4c3247008081855ba7654d04",
      "2b9d2ec6a1e3467b9dd42c0670ccb224",
      "0ee6fd9b77b44ed2b929ce632b71e786",
      "82381b12c1604386b8c1423e38c5833d",
      "bc1a4f1a55fa484ea72fc4e04d19bff4",
      "dc5bd5fd2e9543f1a209eeadc6ea4574",
      "74be0020ec0040cda67f4967f9d3d208",
      "6f91c6f99650407ea054c962e5e07237",
      "ecb01d6919bb43f2918d5d360197b707",
      "80997938d3e54260881812d93bb23d46",
      "54a2c4cef3d8405ea0ba22b4005899f6",
      "2e4f2a527ba449969ba8b56aadf816bb",
      "1ad1731cd6d9420e96cff27eab80a77c"
     ]
    },
    "id": "iVS-ORB_whcV",
    "outputId": "2587e890-fe9d-42ec-b4eb-f1ae2f1a1bdb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0534fcbdca847be89ac18fbf101ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29a3b77bda84ae78173448d4a23bcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67dbc6157464b05993e2ff8e581c2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c8f37345ea4b6292f8f0c8a9f7b97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a4e4cf1a4147c98540c251ce34efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee6fd9b77b44ed2b929ce632b71e786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/253254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3950: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenization\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Longer sequences for complex functions\n",
    "    model_input = tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=768  # Increased from 512\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"target_text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256  # Increased from 128\n",
    "        )\n",
    "\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "# Apply the improved tokenization\n",
    "tokenized_dataset = processed_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "2a049b4e3b5e49bc86f47e39bed8ee0f",
      "8f2ddc7f8bea4dff9b6f8cbab3e990d3",
      "fc2ebd613052447b8e0e3ee14218c078",
      "850985d17c44442e91539d56c9e56bb4",
      "92095f67a65b4b2f962700bdf8330f1f",
      "03d96f17707545f88ad163f3e5369d34",
      "82a5c819632d4a86b68cf7bd4e76af24",
      "fdb8976a15e1422b8b5d683035c8701e",
      "5cd185afe7954e4580dd305c576b6c7a",
      "c71f048132df4a4997ea648eab566d19",
      "f50b57b99df041ec8c197af672368b74"
     ]
    },
    "id": "kdyVA6B2zXIR",
    "outputId": "4393420b-6ef1-44da-a988-ac28fc756915"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a049b4e3b5e49bc86f47e39bed8ee0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/253254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: /content/drive/My Drive/Code Comment Generator/my_tokenized_dataset_enhanced_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path where you want to save it in your Drive\n",
    "save_path = \"/content/drive/My Drive/Code Comment Generator/my_tokenized_dataset_enhanced_2\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save the dataset\n",
    "tokenized_dataset.save_to_disk(save_path)\n",
    "print(f\"Dataset saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4Wcuc5l1C0V"
   },
   "source": [
    "### PHASE 3: Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PotUNl5c36mS"
   },
   "outputs": [],
   "source": [
    "# # load the tokenize dataset from drive\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "# load_path = \"/content/drive/My Drive/Code Comment Generator/my_tokenized_dataset_enhanced\"\n",
    "# tokenized_dataset = load_from_disk(load_path)\n",
    "# print(f\"Dataset loaded from: {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "4256e7722ce24b3c8e84cf2991e5e4e6",
      "ec1cbf4b71104af9acc14ba259d34508",
      "e0fd807722794efe94c1b113c8e39735",
      "26d119fb1026415bad7338661b962bba",
      "5159b5f2ecce4389a0af8a8ed8aea037",
      "2b9273fc748d499ab4d133cf31f57039",
      "c8cd9dd8ad7c4a1db115b63a423177d4",
      "118415af07754353bbf177a4858bcfc8",
      "2cbf62de1863488095b2f4dd180446f2",
      "4638ff0188ba4f2db8ab3b66ca501d97",
      "ef1f39fadbca45de8f0d38ecfc2e7ffd",
      "e62f81fe47ef4ae6bf042fae4c1d35f2",
      "7179e79d9a834ca08ea81f2b10f97894",
      "92d434d4c0d84e338f0d021d54cee775",
      "8db877d399d6490fbec29c6e9c7cbe51",
      "2e6691f44c2b4eab8dea359d8e6a1537",
      "79000429157c472991808395715c0ae9",
      "68d5fbde50e64c9391a6cc73080c2d1c",
      "d975bc2ee10847e8a0f2ce939b5c8b2c",
      "a728cd75876b4760bf02663075e93af1",
      "c2d121958b8444459ca4f6ca49a7aa67",
      "09eadf9c2a37437ba2eeda05c0d65c30",
      "97ba4eb34e9549d99c7e46687316bcb1",
      "75a58fad924e4704bfa90a75e7bf552e",
      "b7be6331e13945689061e4659cf70e20",
      "0fa394922eeb4e5ba769b2875c3a4af3",
      "e1798b5bd1794fd39a8f27593dc8bde2",
      "6850e019e29d42e382d8830fa8bb735d",
      "9d5d53535b724048838ff5659c28820d",
      "fb90916fe00044d88d872f5a4879d0c9",
      "673c11b143a84b57a7cd2e46324f7f84",
      "e068f1d99c154df787a1a13612649058",
      "153c765aef90415086f781a0faf97e6f",
      "779f83fa3b8c43018b1a0a73010ee36f",
      "4777f84e7f2a4c07ba44ecf7f3f89871",
      "0c8feaca7c2642aab0733643f13ed2c2",
      "abd38f44cff24133add4f17ac12d820f",
      "880b3e904a62461eba1cefb58b0b3fb1",
      "fbf4443211af431b8daff1220ba80ce0",
      "fafb5ef957994f6892861da88e0077ba",
      "ba71caa74c2541848b7cd7650ca505e9",
      "ba6cbe129a494534a9382dc220923e8f",
      "c3a1de3348934f11a6b992b5bdfb2fdc",
      "37cabe2f8fba43a1b0d4dd50392884b0"
     ]
    },
    "id": "EROrjO7C86tw",
    "outputId": "f60d4c83-2049-43e4-c742-93ac6e3d3667"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4256e7722ce24b3c8e84cf2991e5e4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62f81fe47ef4ae6bf042fae4c1d35f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ba4eb34e9549d99c7e46687316bcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779f83fa3b8c43018b1a0a73010ee36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    print(f\"Predictions shape: {predictions.shape}, dtype: {predictions.dtype}\")\n",
    "    print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "\n",
    "    try:\n",
    "        # Convert to numpy arrays and ensure correct data type\n",
    "        if isinstance(predictions, torch.Tensor):\n",
    "            predictions = predictions.cpu().numpy()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "        # Ensure the arrays are of integer type and within valid range\n",
    "        predictions = predictions.astype(np.int32)\n",
    "        labels = labels.astype(np.int32)\n",
    "\n",
    "        # For sequence-to-sequence models, predictions might be logits\n",
    "        # If predictions have 3 dimensions, take the argmax to get token IDs\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "        # Clip values to valid token ID range to prevent out-of-range errors\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "        labels = np.clip(labels, 0, vocab_size - 1)\n",
    "\n",
    "        # Replace -100 in labels with pad_token_id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        print(f\"Predictions range: {predictions.min()} to {predictions.max()}\")\n",
    "        print(f\"Labels range: {labels.min()} to {labels.max()}\")\n",
    "        print(f\"Tokenizer vocab size: {vocab_size}\")\n",
    "\n",
    "        # Decode predictions and labels safely\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Clean up any empty strings\n",
    "        decoded_preds = [pred.strip() if pred.strip() else \"empty\" for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() if label.strip() else \"empty\" for label in decoded_labels]\n",
    "\n",
    "        # Debug: Print a few examples\n",
    "        print(f\"Sample predictions: {decoded_preds[:3]}\")\n",
    "        print(f\"Sample labels: {decoded_labels[:3]}\")\n",
    "\n",
    "        # Compute metrics with error handling for each metric\n",
    "        try:\n",
    "            rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            rouge_scores = {\n",
    "                \"rouge1\": round(rouge_result[\"rouge1\"], 2),\n",
    "                \"rouge2\": round(rouge_result[\"rouge2\"], 2),\n",
    "                \"rougeL\": round(rouge_result[\"rougeL\"], 2),\n",
    "            }\n",
    "        except Exception as rouge_error:\n",
    "            print(f\"ROUGE computation failed: {rouge_error}\")\n",
    "            rouge_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "        try:\n",
    "            bleu_result = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "            bleu_score = round(bleu_result[\"bleu\"], 2)\n",
    "        except Exception as bleu_error:\n",
    "            print(f\"BLEU computation failed: {bleu_error}\")\n",
    "            bleu_score = 0.0\n",
    "\n",
    "        final_scores = {\n",
    "            \"bleu\": bleu_score,\n",
    "            **rouge_scores\n",
    "        }\n",
    "\n",
    "        print(f\"Computed metrics: {final_scores}\")\n",
    "        return final_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        # Return default values if computation fails\n",
    "        return {\n",
    "            \"bleu\": 0.0,\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "69f7580698f1421e83ebb58d070eb2cf",
      "d182a059834641708462a3915006dc1b",
      "f84e8a4673964b7b8a4f3c15d492b312",
      "272900a3cb38485db30f73a04b169256",
      "28bfc3b0f1b04cceae50a6f37d700d69",
      "18ab028a239348f28aeb8c1a33258844",
      "f69af2b7dcdb4b4e976c10b5f254c4f4",
      "096c305dfa6a49f7a002af94bfc08fd0",
      "dd82acdf6c804138b3a3e06d95e44c9f",
      "d3e4c3afcf3b4d559746b09421076ab1",
      "66aadf1d5b5748dfaa468158a31e5247",
      "3cb9d9cdb0b9440f8ac598c72ed4ac17",
      "ac9d094dbdad450997dad96be2852f67",
      "58d7b3567dbb4ecdb31c27450c22e780",
      "2d4fd72a21a1456aa0061b33da7df765",
      "ae5c239020214dd299b7a9bc9a823ce1",
      "6781b634e6834a8586f5d5fc4a144bcf",
      "aadf1f00b4d5427abded8cc6a985db3a",
      "8eb07fd462fb4b3f8e5ade5fa17e32ff",
      "e81a281bc000485fbebed687fe08d5bf",
      "5d30b1cabbb44504b6ffac7bfbf983d1",
      "0f76ae39c37d4f868e32a2e3a7fa750e",
      "f37f627cb90949d5b55168f6cf696f45",
      "82ecc293ebaf4219b6b37bb1807f3fa0",
      "8f43d5c4699546b8ad05132a5bed11ec",
      "e6c42e0cdabe48808b38a157a04fc0ea",
      "fb72b14200174b45ac5fe0a1c94f998a",
      "803e8b8ef1974a37a24f0b58d19f1a90",
      "7b95b6aeaaea4dd0b197f0db95ddec97",
      "8ea9bbbf917e4a2ebfdae41861064a28",
      "b7c59ce650ac449593cbb71a6fbd1822",
      "902d5d93ffbe40eea7d3bd9535d14f3c",
      "00fcf52926e34b3aa4694512cf5fdb16"
     ]
    },
    "id": "-pbhgjYAGapb",
    "outputId": "1ccbc91d-a7cc-4208-b5a0-48cd3524838e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7580698f1421e83ebb58d070eb2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb9d9cdb0b9440f8ac598c72ed4ac17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset splits...\n",
      "Training samples: 1000\n",
      "Evaluation samples: 200\n",
      "Creating trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37f627cb90949d5b55168f6cf696f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 11:53, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (200, 256), dtype: int64\n",
      "Labels shape: (200, 256), dtype: int64\n",
      "Predictions range: 0 to 31655\n",
      "Labels range: 0 to 31655\n",
      "Tokenizer vocab size: 32100\n",
      "Sample predictions: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Sample labels: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Computed metrics: {'bleu': 0.88, 'rouge1': np.float64(0.98), 'rouge2': np.float64(0.98), 'rougeL': np.float64(0.98)}\n",
      "Predictions shape: (200, 105), dtype: int64\n",
      "Labels shape: (200, 256), dtype: int64\n",
      "Predictions range: 0 to 31655\n",
      "Labels range: 0 to 31655\n",
      "Tokenizer vocab size: 32100\n",
      "Sample predictions: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Sample labels: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Computed metrics: {'bleu': 0.98, 'rouge1': np.float64(0.99), 'rouge2': np.float64(0.98), 'rougeL': np.float64(0.99)}\n",
      "Predictions shape: (200, 145), dtype: int64\n",
      "Labels shape: (200, 256), dtype: int64\n",
      "Predictions range: 0 to 31655\n",
      "Labels range: 0 to 31655\n",
      "Tokenizer vocab size: 32100\n",
      "Sample predictions: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Sample labels: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Computed metrics: {'bleu': 0.96, 'rouge1': np.float64(0.99), 'rouge2': np.float64(0.98), 'rougeL': np.float64(0.99)}\n",
      "Predictions shape: (200, 118), dtype: int64\n",
      "Labels shape: (200, 256), dtype: int64\n",
      "Predictions range: 0 to 31655\n",
      "Labels range: 0 to 31655\n",
      "Tokenizer vocab size: 32100\n",
      "Sample predictions: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Sample labels: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Computed metrics: {'bleu': 0.99, 'rouge1': np.float64(0.99), 'rouge2': np.float64(0.99), 'rougeL': np.float64(0.99)}\n",
      "Predictions shape: (200, 118), dtype: int64\n",
      "Labels shape: (200, 256), dtype: int64\n",
      "Predictions range: 0 to 31655\n",
      "Labels range: 0 to 31655\n",
      "Tokenizer vocab size: 32100\n",
      "Sample predictions: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Sample labels: ['Increment volume by 0.1 (or delta) unless it is already maxed.\\n        Returns the new volume.', 'backwards compatibility function\\n    :return:', 'select distinct values for a given field for a given a query']\n",
      "Computed metrics: {'bleu': 0.99, 'rouge1': np.float64(0.99), 'rouge2': np.float64(0.99), 'rougeL': np.float64(0.99)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "✅ Training completed successfully!\n",
      "Model saved to: /content/drive/MyDrive/codet5-small-comment-generator-enhanced\n",
      "Training process completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Create dataset splits with larger sizes\n",
    "print(\"Creating dataset splits...\")\n",
    "train_size = 1000\n",
    "eval_size = 200\n",
    "\n",
    "# Make sure tokenized_dataset is available\n",
    "shuffled_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "train_dataset = shuffled_dataset.select(range(train_size))\n",
    "eval_dataset = shuffled_dataset.select(range(train_size, train_size + eval_size))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Enhanced Training arguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/codet5-small-training-enhanced\"\n",
    "model_save_dir = \"/content/drive/MyDrive/codet5-small-comment-generator-enhanced\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=3e-4,  # Higher learning rate\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{output_dir}/logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,    # Less frequent saving\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_bleu\",  # Use ROUGE instead of loss\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,  # Simulate larger batch size\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    # Valid generation parameters for evaluation\n",
    "    generation_max_length=256,  # Increased\n",
    "    generation_num_beams=6,     # More beams\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "print(\"Creating trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Keep your existing compute_metrics function\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "try:\n",
    "    training_history = trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(model_save_dir)\n",
    "    tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    print(f\"Model saved to: {model_save_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # Save whatever progress was made\n",
    "    try:\n",
    "        trainer.save_model(f\"{model_save_dir}_partial\")\n",
    "        tokenizer.save_pretrained(f\"{model_save_dir}_partial\")\n",
    "        print(f\"Partial model saved to: {model_save_dir}_partial\")\n",
    "    except Exception as save_error:\n",
    "        print(f\"Failed to save partial model: {save_error}\")\n",
    "\n",
    "print(\"Training process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_biQ12AErBp"
   },
   "source": [
    "### Testing the fine-tuned model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-uM5H5GjcfQ",
    "outputId": "80285235-ceba-4d5d-e551-571ede9a46b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating enhanced docstrings for all test cases...\n",
      "Total test cases: 20\n",
      "====================================================================================================\n",
      "\n",
      "=== Test Case 1 ===\n",
      "Code: Generate docstring: def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "Enhanced Docstring: 1\n",
      "    else:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 2 ===\n",
      "Code: Generate docstring: def is_prime(num):\n",
      "    if num < 2:\n",
      "        return False\n",
      "    for i in range(2, int(num ** 0.5) + 1):\n",
      "        if num % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "Enhanced Docstring: num < 2:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 3 ===\n",
      "Code: Generate docstring: def reverse_string(s):\n",
      "    return s[::-1]\n",
      "Enhanced Docstring: return s[::-1]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 4 ===\n",
      "Code: Generate docstring: def count_words(text):\n",
      "    words = text.split()\n",
      "    return len(words)\n",
      "Enhanced Docstring: Generate docstring for the text.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 5 ===\n",
      "Code: Generate docstring: def find_max(lst):\n",
      "    if not lst:\n",
      "        return None\n",
      "    max_val = lst[0]\n",
      "    for val in lst[1:]:\n",
      "        if val > max_val:\n",
      "            max_val = val\n",
      "    return max_val\n",
      "Enhanced Docstring: max_val = lst[0]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 6 ===\n",
      "Code: Generate docstring: def binary_search(arr, target):\n",
      "    left, right = 0, len(arr) - 1\n",
      "    while left <= right:\n",
      "        mid = (left + right) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            left = mid + 1\n",
      "        else:\n",
      "            right = mid - 1\n",
      "    return -1\n",
      "Enhanced Docstring: left, right = 0, len(arr) - 1\n",
      "    while left <= right:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 7 ===\n",
      "Code: Generate docstring: def merge_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    mid = len(arr) // 2\n",
      "    left = merge_sort(arr[:mid])\n",
      "    right = merge_sort(arr[mid:])\n",
      "    return merge(left, right)\n",
      "Enhanced Docstring: return arr\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 8 ===\n",
      "Code: Generate docstring: def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    a, b = 0, 1\n",
      "    for _ in range(2, n + 1):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "Enhanced Docstring: n\n",
      "    a, b = 0, 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 9 ===\n",
      "Code: Generate docstring: def is_palindrome(s):\n",
      "    s = ''.join(char.lower() for char in s if char.isalnum())\n",
      "    return s == s[::-1]\n",
      "Enhanced Docstring: palindrome\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 10 ===\n",
      "Code: Generate docstring: def flatten_list(nested_list):\n",
      "    result = []\n",
      "    for item in nested_list:\n",
      "        if isinstance(item, list):\n",
      "            result.extend(flatten_list(item))\n",
      "        else:\n",
      "            result.append(item)\n",
      "    return result\n",
      "Enhanced Docstring: result = []\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 11 ===\n",
      "Code: Generate docstring: def calculate_gcd(a, b):\n",
      "    while b:\n",
      "        a, b = b, a % b\n",
      "    return a\n",
      "Enhanced Docstring: while b:\n",
      "        a, b = b, a % b\n",
      "    return a % b\n",
      "   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 12 ===\n",
      "Code: Generate docstring: def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quick_sort(left) + middle + quick_sort(right)\n",
      "Enhanced Docstring: return arr\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 13 ===\n",
      "Code: Generate docstring: def validate_email(email):\n",
      "    import re\n",
      "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
      "    return re.match(pattern, email) is not None\n",
      "Enhanced Docstring: [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 14 ===\n",
      "Code: Generate docstring: def remove_duplicates(lst):\n",
      "    seen = set()\n",
      "    result = []\n",
      "    for item in lst:\n",
      "        if item not in seen:\n",
      "            seen.add(item)\n",
      "            result.append(item)\n",
      "    return result\n",
      "Enhanced Docstring: seen set()\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 15 ===\n",
      "Code: Generate docstring: def matrix_multiply(A, B):\n",
      "    rows_A, cols_A = len(A), len(A[0])\n",
      "    rows_B, cols_B = len(B), len(B[0])\n",
      "    if cols_A != rows_B:\n",
      "        raise ValueError(\"Cannot multiply matrices\")\n",
      "    result = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
      "    for i in range(rows_A):\n",
      "        for j in range(cols_B):\n",
      "            for k in range(cols_A):\n",
      "                result[i][j] += A[i][k] * B[k][j]\n",
      "    return result\n",
      "Enhanced Docstring: rows_A, cols_A = len(A), len(A[0])\n",
      "    rows_B, cols_B = len(B[0])\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 16 ===\n",
      "Code: Generate docstring: def decode_base64(encoded_string):\n",
      "    import base64\n",
      "    try:\n",
      "        decoded_bytes = base64.b64decode(encoded_string)\n",
      "        return decoded_bytes.decode('utf-8')\n",
      "    except Exception:\n",
      "        return None\n",
      "Enhanced Docstring: import base64.b64decode(encoded_string)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 17 ===\n",
      "Code: Generate docstring: def find_common_elements(list1, list2):\n",
      "    return list(set(list1) & set(list2))\n",
      "Enhanced Docstring: return list(set(list1) & set(list2))\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 18 ===\n",
      "Code: Generate docstring: def calculate_compound_interest(principal, rate, time, n):\n",
      "    amount = principal * (1 + rate / n) ** (n * time)\n",
      "    return round(amount - principal, 2)\n",
      "Enhanced Docstring: amount - principal * (1 + rate / n) ** (n * time)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 19 ===\n",
      "Code: Generate docstring: def parse_json_safe(json_string):\n",
      "    import json\n",
      "    try:\n",
      "        return json.loads(json_string)\n",
      "    except json.JSONDecodeError:\n",
      "        return {}\n",
      "Enhanced Docstring: import json.loads(json_string)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Test Case 20 ===\n",
      "Code: Generate docstring: def levenshtein_distance(s1, s2):\n",
      "    if len(s1) < len(s2):\n",
      "        return levenshtein_distance(s2, s1)\n",
      "    if len(s2) == 0:\n",
      "        return len(s1)\n",
      "    previous_row = list(range(len(s2) + 1))\n",
      "    for i, c1 in enumerate(s1):\n",
      "        current_row = [i + 1]\n",
      "        for j, c2 in enumerate(s2):\n",
      "            insertions = previous_row[j + 1] + 1\n",
      "            deletions = current_row[j] + 1\n",
      "            substitutions = previous_row[j] + (c1 != c2)\n",
      "            current_row.append(min(insertions, deletions, substitutions))\n",
      "        previous_row = current_row\n",
      "    return previous_row[-1]\n",
      "Enhanced Docstring: return levenshtein_distance(s2, s1)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "✅ All 20 test cases completed!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Complete test suite with all 20 test cases\n",
    "all_test_inputs = [\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def is_prime(num):\n",
    "    if num < 2:\n",
    "        return False\n",
    "    for i in range(2, int(num ** 0.5) + 1):\n",
    "        if num % i == 0:\n",
    "            return False\n",
    "    return True\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def reverse_string(s):\n",
    "    return s[::-1]\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def find_max(lst):\n",
    "    if not lst:\n",
    "        return None\n",
    "    max_val = lst[0]\n",
    "    for val in lst[1:]:\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "    return max_val\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def merge_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])\n",
    "    right = merge_sort(arr[mid:])\n",
    "    return merge(left, right)\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    a, b = 0, 1\n",
    "    for _ in range(2, n + 1):\n",
    "        a, b = b, a + b\n",
    "    return b\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def is_palindrome(s):\n",
    "    s = ''.join(char.lower() for char in s if char.isalnum())\n",
    "    return s == s[::-1]\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def flatten_list(nested_list):\n",
    "    result = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten_list(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def calculate_gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a % b\n",
    "    return a\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quick_sort(left) + middle + quick_sort(right)\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def validate_email(email):\n",
    "    import re\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return re.match(pattern, email) is not None\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def remove_duplicates(lst):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def matrix_multiply(A, B):\n",
    "    rows_A, cols_A = len(A), len(A[0])\n",
    "    rows_B, cols_B = len(B), len(B[0])\n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Cannot multiply matrices\")\n",
    "    result = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            for k in range(cols_A):\n",
    "                result[i][j] += A[i][k] * B[k][j]\n",
    "    return result\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def decode_base64(encoded_string):\n",
    "    import base64\n",
    "    try:\n",
    "        decoded_bytes = base64.b64decode(encoded_string)\n",
    "        return decoded_bytes.decode('utf-8')\n",
    "    except Exception:\n",
    "        return None\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def find_common_elements(list1, list2):\n",
    "    return list(set(list1) & set(list2))\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def calculate_compound_interest(principal, rate, time, n):\n",
    "    amount = principal * (1 + rate / n) ** (n * time)\n",
    "    return round(amount - principal, 2)\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def parse_json_safe(json_string):\n",
    "    import json\n",
    "    try:\n",
    "        return json.loads(json_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"\"\"Generate docstring: def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Enhanced generation with better parameters\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "generator = pipeline(\"text2text-generation\",\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)\n",
    "\n",
    "# Generate summaries with enhanced parameters for all 20 test cases\n",
    "print(\"Generating enhanced docstrings for all test cases...\")\n",
    "print(f\"Total test cases: {len(all_test_inputs)}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, item in enumerate(all_test_inputs, 1):\n",
    "    print(f\"\\n=== Test Case {i} ===\")\n",
    "\n",
    "    # Method 1: Using pipeline with better parameters\n",
    "    output = generator(\n",
    "        item[\"input_text\"],\n",
    "        max_new_tokens=150,     # Use max_new_tokens instead of max_length\n",
    "        num_beams=6,            # More beams for better search\n",
    "        early_stopping=True,\n",
    "        do_sample=True,         # Add some controlled randomness\n",
    "        temperature=1,          # Control randomness\n",
    "        top_p=0.5,              # Nucleus sampling\n",
    "        repetition_penalty=1.2, # Avoid repetition\n",
    "        length_penalty=1.0      # Encourage longer outputs\n",
    "    )\n",
    "\n",
    "    print(\"Code:\", item[\"input_text\"])\n",
    "    print(\"Enhanced Docstring:\", output[0]['generated_text'])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✅ All 20 test cases completed!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP43d5X6OqqX"
   },
   "source": [
    "I tried using few-shot prompting to get a better output without succeeding\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
